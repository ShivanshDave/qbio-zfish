{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Colab_TrainNetwork_VideoAnalysis.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/AlexEMG/DeepLabCut/blob/master/examples/Colab_TrainNetwork_VideoAnalysis.ipynb","timestamp":1548448850661}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"RK255E7YoEIt","colab_type":"text"},"cell_type":"markdown","source":["# DeepLabCut Toolbox - Colab\n","https://github.com/AlexEMG/DeepLabCut\n","\n","Nath\\*, Mathis\\* et al. *Using DeepLabCut for markerless pose estimation during behavior across species*, (under revision).\n","\n","This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n","This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n","\n","This notebook illustrates how to use the cloud to:\n","- train a network\n","- evaluate a network\n","- analyze a novel video\n","\n","This assumes you already have a project folder with labeled data! \n"]},{"metadata":{"id":"txoddlM8hLKm","colab_type":"text"},"cell_type":"markdown","source":["## Let's look at info about the Colab Environment:\n"]},{"metadata":{"id":"4C5WRoS9g5Od","colab_type":"code","colab":{}},"cell_type":"code","source":["!nvcc --version\n","import tensorflow as tf\n","tf.__version__\n","tf.test.gpu_device_name()\n","#from tensorflow.python.client import device_lib\n","#device_lib.list_local_devices()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KS4Q4UkR9rgG","colab_type":"code","colab":{}},"cell_type":"code","source":["#Colab only - to link to your GoogleDrive, run this cell and follow the authorization instructions:\n","#(We recommend putting a copy of github repo as a folder in your google drive)\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q23BzhA6CXxu","colab_type":"code","colab":{}},"cell_type":"code","source":["#--# NEW - CHANGE\n","\n","#in colab: go to your deeplabcut folder --- TODO - get 'better' deeplabcut repo\n","%cd /content/drive/My Drive/ColabFiles/qbio\n","\n","#You'll need to uncomment a few lines of code to make the notebook work:\n","#(this will take a few minutes to install all the dependences!)\n","\n","!pip install deeplabcut\n","!pip install --no-cache-dir -I pillow\n","!pip install pillow==4.1.1\n","%reload_ext autoreload\n","%autoreload"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sXufoX6INe6w","colab_type":"code","colab":{}},"cell_type":"code","source":["#GUIs don't work on the cloud, so label your data locally on your computer! \n","import os\n","os.environ[\"DLClight\"]=\"True\"\n","os.environ[\"Colab\"]=\"True\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"3K9Ndy1beyfG","colab_type":"code","colab":{}},"cell_type":"code","source":["import deeplabcut\n","\n","#sometimes the backend kernel is defaulting to something other than what we want\n","#if you see the warnings, you can re-run this cell and they go way, as we re-set the backend! :) \n","\n","#possible warning--> UserWarning: This call to matplotlib.use() has no effect because the backend has already\n","#been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot, or matplotlib.backends is imported for the first time."],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z7ZlDr3wV4D1","colab_type":"code","colab":{}},"cell_type":"code","source":["#--# NEW - edit path -- in project.config.yaml\n","#--# RESTART - edit -- dlc-models/../train/pose_cfg.yaml\n","\n","#--# NEW - edit path\n","path_config_file = '/content/drive/My Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/config.yaml'\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Frnj1RVDyEqs","colab_type":"text"},"cell_type":"markdown","source":["YOU WILL NEED TO EDIT THE PROJECT PATH **in the config.yaml file** TO BE SET TO YOUR GOOGLE DRIVE LINK!\n"]},{"metadata":{"id":"xNi9s1dboEJN","colab_type":"text"},"cell_type":"markdown","source":["## Create a training dataset:\n","### we recommend doing this outside of Colab, then copying your file into Drive\n","This function generates the training data information for DeepCut (which requires a mat file) based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file. While creating the dataset, the user can create multiple shuffles. \n","\n","After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n","\n","This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n","\n","Now it is the time to start training the network!"]},{"metadata":{"scrolled":true,"id":"eMeUwgxPoEJP","colab_type":"code","colab":{}},"cell_type":"code","source":["#If you didn't run this already...\n","#And if you DID, you may get errors, as you need to delete the created folder first - they will not be overwritten! (i.e. delete \"dlc-models\")\n","#deeplabcut.create_training_dataset(path_config_file,Shuffles=[1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c4FczXGDoEJU","colab_type":"text"},"cell_type":"markdown","source":["## Start training\n","This function trains the network for a specific shuffle of the training dataset. "]},{"metadata":{"id":"_pOvDq_2oEJW","colab_type":"code","outputId":"3780cd44-537e-41f5-84d2-779ad197ec31","colab":{"base_uri":"https://localhost:8080/","height":2145}},"cell_type":"code","source":["#let's also change the display and save_iters just in case Colab takes away the GPU... \n","#if that happens, you can reload from a saved point. Typically, you want to train to 200,000 + iterations.\n","#more infor, and there are more things you can set: https://github.com/AlexEMG/DeepLabCut/blob/master/docs/functionDetails.md#g-train-the-network\n","\n","deeplabcut.train_network(path_config_file, shuffle=1, displayiters=100,saveiters=5000)\n","\n","#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations). \n","#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry...."],"execution_count":0,"outputs":[{"output_type":"stream","text":["Config:\n","{'all_joints': [[0], [1], [2], [3]],\n"," 'all_joints_names': ['head', 'tail', 'up', 'down'],\n"," 'batch_size': 1,\n"," 'bottomheight': 400,\n"," 'crop': True,\n"," 'crop_pad': 0,\n"," 'cropratio': 0.4,\n"," 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SwimmingJan25/Swimming_Shivansh95shuffle1.mat',\n"," 'dataset_type': 'default',\n"," 'display_iters': 1000,\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 0.8,\n"," 'init_weights': '/content/drive/My '\n","                 'Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/dlc-models/iteration-0/SwimmingJan25-trainset95shuffle1/train/snapshot-175000',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'leftwidth': 400,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 0.05,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'max_input_size': 1000,\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SwimmingJan25/Documentation_data-Swimming_95shuffle1.pickle',\n"," 'minsize': 100,\n"," 'mirror': False,\n"," 'multi_step': [[0.005, 10000],\n","                [0.02, 430000],\n","                [0.002, 730000],\n","                [0.001, 1030000]],\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 4,\n"," 'optimizer': 'sgd',\n"," 'pos_dist_thresh': 17,\n"," 'project_path': '/content/drive/My '\n","                 'Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25',\n"," 'regularize': False,\n"," 'rightwidth': 400,\n"," 'save_iters': 50000,\n"," 'scale_jitter_lo': 0.5,\n"," 'scale_jitter_up': 1.25,\n"," 'scoremap_dir': 'test',\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/My '\n","                    'Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/dlc-models/iteration-0/SwimmingJan25-trainset95shuffle1/train/snapshot',\n"," 'stride': 8.0,\n"," 'topheight': 400,\n"," 'use_gt_segm': False,\n"," 'video': False,\n"," 'video_batch': False,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n","Config:\n","{'all_joints': [[0], [1], [2], [3]],\n"," 'all_joints_names': ['head', 'tail', 'up', 'down'],\n"," 'batch_size': 1,\n"," 'bottomheight': 400,\n"," 'crop': True,\n"," 'crop_pad': 0,\n"," 'cropratio': 0.4,\n"," 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SwimmingJan25/Swimming_Shivansh95shuffle1.mat',\n"," 'dataset_type': 'default',\n"," 'display_iters': 1000,\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 0.8,\n"," 'init_weights': '/content/drive/My '\n","                 'Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/dlc-models/iteration-0/SwimmingJan25-trainset95shuffle1/train/snapshot-175000',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'leftwidth': 400,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 0.05,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'max_input_size': 1000,\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SwimmingJan25/Documentation_data-Swimming_95shuffle1.pickle',\n"," 'minsize': 100,\n"," 'mirror': False,\n"," 'multi_step': [[0.005, 10000],\n","                [0.02, 430000],\n","                [0.002, 730000],\n","                [0.001, 1030000]],\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 4,\n"," 'optimizer': 'sgd',\n"," 'pos_dist_thresh': 17,\n"," 'project_path': '/content/drive/My '\n","                 'Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25',\n"," 'regularize': False,\n"," 'rightwidth': 400,\n"," 'save_iters': 50000,\n"," 'scale_jitter_lo': 0.5,\n"," 'scale_jitter_up': 1.25,\n"," 'scoremap_dir': 'test',\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/My '\n","                    'Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/dlc-models/iteration-0/SwimmingJan25-trainset95shuffle1/train/snapshot',\n"," 'stride': 8.0,\n"," 'topheight': 400,\n"," 'use_gt_segm': False,\n"," 'video': False,\n"," 'video_batch': False,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/My Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/dlc-models/iteration-0/SwimmingJan25-trainset95shuffle1/train/snapshot-175000\n"],"name":"stdout"},{"output_type":"stream","text":["Restoring parameters from /content/drive/My Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/dlc-models/iteration-0/SwimmingJan25-trainset95shuffle1/train/snapshot-175000\n","Restoring parameters from /content/drive/My Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/dlc-models/iteration-0/SwimmingJan25-trainset95shuffle1/train/snapshot-175000\n"],"name":"stderr"},{"output_type":"stream","text":["Display_iters overwritten as 100\n","Save_iters overwritten as 5000\n","Training parameter:\n","{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'weigh_only_present_joints': False, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/dlc-models/iteration-0/SwimmingJan25-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'mirror': False, 'crop_pad': 0, 'scoremap_dir': 'test', 'dataset_type': 'default', 'use_gt_segm': False, 'batch_size': 1, 'video': False, 'video_batch': False, 'crop': True, 'cropratio': 0.4, 'minsize': 100, 'leftwidth': 400, 'rightwidth': 400, 'topheight': 400, 'bottomheight': 400, 'all_joints': [[0], [1], [2], [3]], 'all_joints_names': ['head', 'tail', 'up', 'down'], 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SwimmingJan25/Swimming_Shivansh95shuffle1.mat', 'display_iters': 1000, 'init_weights': '/content/drive/My Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25/dlc-models/iteration-0/SwimmingJan25-trainset95shuffle1/train/snapshot-175000', 'max_input_size': 1000, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_SwimmingJan25/Documentation_data-Swimming_95shuffle1.pickle', 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 4, 'pos_dist_thresh': 17, 'project_path': '/content/drive/My Drive/ColabFiles/qbio/track-exp5-6__Swimming-Shivansh-2019-01-25', 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25}\n","Starting training....\n"],"name":"stdout"},{"output_type":"stream","text":["iteration: 100 loss: 0.0322 lr: 0.005\n","iteration: 100 loss: 0.0322 lr: 0.005\n","iteration: 200 loss: 0.0092 lr: 0.005\n","iteration: 200 loss: 0.0092 lr: 0.005\n"],"name":"stderr"}]},{"metadata":{"id":"RiDwIVf5-3H_","colab_type":"text"},"cell_type":"markdown","source":["**When you hit \"STOP\" you will get a KeyInterrupt \"error\"! No worries! :)**"]},{"metadata":{"id":"xZygsb2DoEJc","colab_type":"text"},"cell_type":"markdown","source":["## Start evaluating\n","This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n","and stores the results as .csv file in a subdirectory under **evaluation-results**"]},{"metadata":{"id":"flnaqXzcBZ_X","colab_type":"code","colab":{}},"cell_type":"code","source":["#this is a colab specific work-around, but it works!\n","\n","!pip install Pillow==4.0.0\n","\n","from PIL import Image\n","def register_extension(id, extension): Image.EXTENSION[extension.lower()] = id.upper()\n","Image.register_extension = register_extension\n","def register_extensions(id, extensions): \n","  for extension in extensions: register_extension(id, extension)\n","Image.register_extensions = register_extensions"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nv4zlbrnoEJg","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","\n","deeplabcut.evaluate_network(path_config_file,Shuffles=[1],plotting=True)\n","\n","# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, so be sure your labels are good!"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BaLBl3TQtrfB","colab_type":"text"},"cell_type":"markdown","source":["## There is an optional refinement step\n","- if your pixel errors are not low enough, please check out the protocol guide on how to refine your network!\n","- You will need to adjust the labels **outside of Colab!** We recommend coming back to train and analyze videos... \n","- pplease see the repo and protocol instructions on how to refine your data!"]},{"metadata":{"id":"OVFLSKKfoEJk","colab_type":"text"},"cell_type":"markdown","source":["## Start Analyzing videos\n","This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n","\n","The results are stored in hd5 file in the same directory where the video resides. "]},{"metadata":{"id":"Y_LZiS_0oEJl","colab_type":"code","colab":{}},"cell_type":"code","source":["videofile_path = ['/content/drive/My Drive/DeepLabCut/examples/Reaching-Mackenzie-2018-08-30/videos/MovieS2_Perturbation_noLaser_compressed.avi'] #Enter the list of videos to analyze.\n","deeplabcut.analyze_videos(path_config_file,videofile_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pCrUvQIvoEKD","colab_type":"text"},"cell_type":"markdown","source":["## Create labeled video\n","This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "]},{"metadata":{"id":"6aDF7Q7KoEKE","colab_type":"code","colab":{}},"cell_type":"code","source":["deeplabcut.create_labeled_video(path_config_file,videofile_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8GTiuJESoEKH","colab_type":"text"},"cell_type":"markdown","source":["## Plot the trajectories of the analyzed videos\n","This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."]},{"metadata":{"id":"gX21zZbXoEKJ","colab_type":"code","colab":{}},"cell_type":"code","source":["#for making interactive plots.\n","%matplotlib notebook \n","deeplabcut.plot_trajectories(path_config_file,videofile_path)"],"execution_count":0,"outputs":[]}]}